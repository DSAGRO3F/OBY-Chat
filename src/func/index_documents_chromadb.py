"""
    Outils d‚Äôindexation ChromaDB pour OBY-IA.

    Ce module expose des utilitaires pour (r√©)indexer des collections ChromaDB
    √† partir de r√©pertoires de JSON structur√©s :
    - `base_docx` : documents d√©riv√©s de fiches DOCX,
    - `base_web`  : documents d√©riv√©s du scraping de sites de confiance.

    Fournit notamment une fonction de reconstruction qui
    supprime la collection cibl√©e puis la reconstruit √† partir des fichiers
    pr√©sents sur disque, garantissant l‚Äôabsence de documents ¬´ fant√¥mes ¬ª
    lorsqu‚Äôil y a des suppressions ou des changements de configuration.

    Fonctions attendues dans ce module (ou import√©es) :
    - `index_documents(source_dir, source_type, client)`: effectue l‚Äôindexation
      √† partir d‚Äôun r√©pertoire JSON (cr√©e la collection si n√©cessaire).
    - `collection_name_for(source_type)`: mappe 'docx'/'web' vers le nom
      de collection ChromaDB (p. ex. 'base_docx' / 'base_web').
    - `rebuild_collection_from_disk(client, source_type, source_dir)`: supprime
      la collection puis r√©indexe depuis le disque (cf. docstring ci-dessous).

"""


from uuid import uuid4
from datetime import datetime
import os, json
from chromadb.api import ClientAPI
import hashlib
from pathlib import Path
from urllib.parse import urlparse, urljoin, urldefrag

from chromadb.utils import embedding_functions
from src.utils.chroma_client import get_chroma_client

from config.config import EMBEDDING_MODEL_NAME

# ============================================================#
# --- Pour √©viter de remonter des None dans metadata/chroma---#
# ============================================================#

def _safe_str(v, default=""):
    # Si v est None ou cha√Æne vide, retourne default
    if v is None:
        return default
    if isinstance(v, str):
        return v if v.strip() else default
    return str(v)


def _sanitize_meta(d: dict) -> dict:
    """Garde uniquement Bool/Int/Float/Str, enl√®ve les None et vide."""
    out = {}
    for k, v in d.items():
        if v is None:
            continue
        if isinstance(v, (bool, int, float)):
            out[k] = v
        elif isinstance(v, str):
            vv = v.strip()
            if vv != "":
                out[k] = vv
        else:
            # si c'est un autre type (list/dict/None), on le convertit en str propre
            s = str(v).strip()
            if s:
                out[k] = s
    return out


# ============================================================#
# ---Pour arriver √† capturer les url valides pour metadata ---#
# ============================================================#

CANDIDATE_URL_KEYS = ["source_url", "url", "page_url", "canonical_url", "canonical", "href", "permalink"]
CANDIDATE_BASE_KEYS = ["base_url", "site_url", "site", "origin"]

def _first_key(d: dict, keys: list[str]) -> str | None:
    for k in keys:
        v = d.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return None

def _is_http_url(u: str | None) -> bool:
    return isinstance(u, str) and u.lower().startswith(("http://", "https://"))

def _infer_domain_from_filename(file: str) -> str | None:
    # ex: "www.has-sante.fr_jcms_c_1718248_fr_....json" -> "www.has-sante.fr"
    head = file.split("_", 1)[0]
    return head if "." in head else None

def _normalize_abs_url(raw_url: str | None, *, file: str, fiche: dict) -> tuple[str | None, str | None]:
    """Retourne (url_absolue_normalis√©e, domaine) ou (None, None)."""
    if not raw_url:
        return None, None
    raw = raw_url.strip()

    # URLs type "//example.com/..." -> les rendre absolues
    if raw.startswith("//"):
        raw = "https:" + raw

    # Cas d√©j√† absolu
    if _is_http_url(raw):
        url, _frag = urldefrag(raw)          # enl√®ve l‚Äôancre
        if url.endswith("/"):
            url = url[:-1]                   # normalise sans slash final
        return url, urlparse(url).netloc

    # Cas relatif -> tenter une base
    base = _first_key(fiche, CANDIDATE_BASE_KEYS)
    if base and _is_http_url(base):
        absu = urljoin(base.rstrip("/") + "/", raw.lstrip("/"))
        url, _ = urldefrag(absu)
        if url.endswith("/"):
            url = url[:-1]
        return url, urlparse(url).netloc

    # Dernier recours : d√©duire le domaine depuis le nom du fichier
    dom = _infer_domain_from_filename(file)
    if dom:
        absu = f"https://{dom}/{raw.lstrip('/')}"
        url, _ = urldefrag(absu)
        if url.endswith("/"):
            url = url[:-1]
        return url, dom

    return None, None



def _collection_name_for(source_type: str) -> str:
    """Retourne le nom de collection ChromaDB pour un `source_type` donn√©.

        Args:
            source_type: 'docx' ou 'web'.

        Returns:
            Nom de collection (p. ex. 'base_docx' ou 'base_web').

        Raises:
            ValueError: si `source_type` n‚Äôest pas 'docx' ni 'web'.
    """

    if source_type not in {"docx", "web"}:
        raise ValueError(f"source_type invalide: {source_type!r} (attendus: 'docx' ou 'web')")
    return "base_docx" if source_type == "docx" else "base_web"



def rebuild_collection_from_disk(client: ClientAPI, source_type: str, source_dir: str) -> None:
    """
    Reconstruit enti√®rement la collection ChromaDB d‚Äôun type donn√©.

    Objectif: garantir la coh√©rence parfaite entre l‚Äô√©tat
    disque (r√©pertoire JSON) et l‚Äôindex ChromaDB (par ex. apr√®s suppressions
    de fichiers, changements de configuration des sites, migration d‚Äôembedding,
    etc.).

    1) supprime la collection cibl√©e (si elle existe),
    2) (re)cr√©e et r√©indexe la collection en appelant `index_documents`
       √† partir des JSON pr√©sents dans `source_dir`.

    Args:
        client: instance ChromaDB (ClientAPI) d√©j√† initialis√©e.
        source_type: 'docx' ou 'web' (d√©termine la collection √† reconstruire).
        source_dir: chemin du r√©pertoire contenant les JSON √† indexer.

    Raises:
        ValueError: si `source_type` n‚Äôest pas 'docx' ni 'web'.
        Exception: si la suppression ou la r√©indexation √©choue (erreurs du client
            ChromaDB ou d‚ÄôE/S remont√©es telles quelles).

    Returns:
        None

    """

    name = _collection_name_for(source_type)
    try:
        client.delete_collection(name=name)
        print(f"üî¥ Collection supprim√©e : {name}")
    except Exception as e:
        print(f"üî¥ Impossible de supprimer (peut-√™tre absente) {name} : {e}")
    # R√©indexation compl√®te depuis le r√©pertoire (cr√©e la collection si besoin)
    index_documents(source_dir=source_dir, source_type=source_type, client=client)
    print(f"‚úÖ Collection reconstruite : {name}")


def index_documents(source_dir: str, source_type: str, client: ClientAPI):
    """
    Indexe les documents JSON contenus dans un r√©pertoire dans une collection ChromaDB.

    Chaque document est d√©coup√© en sections (ou chunk unique dans le cas d'un fichier DOCX complet),
    puis ins√©r√© dans une base vectorielle avec ses m√©tadonn√©es.

    Args:
        source_dir (str): Chemin du dossier contenant les fichiers JSON √† indexer.
        source_type (str): Type de document √† indexer, soit 'docx' soit 'web'.
        client (Client): Instance du client ChromaDB utilis√©e pour la persistance des donn√©es.

    Entr√©es :
        - source_dir (str) : Dossier contenant les fichiers JSON.
        - source_type (str) : 'docx' ou 'web' (d√©termine la collection cible).

    Sorties :
        - Indexation des chunks dans une collection nomm√©e selon la source.


    Raises:
        ValueError: Si le type de source est invalide (autre que 'docx' ou 'web').
    """

    print("üü° Lancement fonction -> index_documents()... ")
    if client is None:
        client = get_chroma_client()

    if source_type not in ("docx", "web"):
        raise ValueError("source_type doit √™tre 'docx' ou 'web'.")

    collection_name = "base_docx" if source_type == "docx" else "base_web"

    # üîπ Initialisation collection
    print(f'üü°Initialisation de la collection {collection_name}...')
    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL_NAME
    )

    collection = client.get_or_create_collection(name=collection_name, embedding_function=embedding_fn)

    print(f"üü°Indexation des documents depuis : {source_dir} (type: {source_type})")

    total_files, indexed_chunks = 0, 0

    for file in os.listdir(source_dir):
        if not file.endswith(".json"):
            continue
        print(f"üü° Traitement du fichier JSON : {file}")

        file_path = os.path.join(source_dir, file)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"‚ùå Erreur de lecture {file}: {e}")
            continue

        fiches = data if isinstance(data, list) else [data]

        for fiche in fiches:
            print(
                f"‚û°Ô∏è Titre fiche : {fiche.get('titre', 'Sans titre')} - Source : {fiche.get('source_url', 'inconnue')}")

            if source_type == "docx":
                chunk_text = (fiche.get("texte_complet") or "").strip()
                if not chunk_text:
                    print(f"‚ö†Ô∏è Texte vide pour le fichier {file}, source_type : {source_type}")
                    continue

                titre = _safe_str(fiche.get("titre"), "Titre inconnu")
                type_document = _safe_str(fiche.get("type_document"), "document")
                source = _safe_str(fiche.get("source_doc"), "source_docx_inconnue")

                chunk_id = f"{file}_{uuid4().hex[:8]}"
                basename = Path(file).name
                fiche_id = hashlib.md5(f"{source}|{titre}".encode("utf-8")).hexdigest()

                meta = {
                    "titre": titre,
                    "type_document": type_document,
                    "source": source,
                    "source_group": source,
                    "source_path": str(file),
                    "source_basename": basename,
                    "fiche_id": fiche_id,
                    "section_index": 0,
                    "source_type": "docx",
                    "date_indexation": datetime.now().strftime("%Y-%m-%d"),
                }
                meta = _sanitize_meta(meta)

                try:
                    collection.add(documents=[chunk_text], ids=[chunk_id], metadatas=[meta])
                    indexed_chunks += 1
                except Exception as e:
                    print(f"‚ùå Erreur d‚Äôajout depuis {file} : {e}\n[DEBUG meta]={meta}")

            elif source_type == "web":
                sections = fiche.get("sections") or []
                if not sections:
                    print(f"‚ö†Ô∏è Aucune section trouv√©e dans le fichier {file}, source_type : {source_type}")
                    continue

                # 1) extraire une URL brute parmis plusieurs cl√©s possibles
                raw_url = _first_key(fiche, CANDIDATE_URL_KEYS)

                # 2) calculer une URL absolue + domaine
                url_val, domain = _normalize_abs_url(raw_url, file=file, fiche=fiche)

                titre = _safe_str(fiche.get("titre"), "Sans titre")
                type_document = _safe_str(fiche.get("type_document"), "page_web")
                source_for_histo = url_val or "Source inconnue"

                if not url_val:
                    # log pour debug: quelles cl√©s pr√©sentait la fiche ?
                    print(f"üü° [WEB] Pas d'URL exploitable pour {file} ; cl√©s dispo: "
                          f"{[k for k in ['source_url', 'url', 'page_url', 'canonical_url', 'canonical', 'href', 'permalink', 'base_url'] if fiche.get(k)]}")

                for i, section in enumerate(sections):
                    chunk_text = (section.get("texte") or "").strip()
                    if not chunk_text:
                        print(f"‚ö†Ô∏è Chunk vide dans section {i} du fichier {file}")
                        continue

                    chunk_id = f"{file}_{uuid4().hex[:8]}"

                    meta = {
                        "titre": titre,
                        "type_document": type_document,
                        "source": source_for_histo,  # compat historique
                        "section_index": int(i),
                        "source_type": "web",
                        "date_indexation": datetime.now().strftime("%Y-%m-%d"),
                    }
                    if url_val:
                        meta["url"] = url_val
                    if domain:
                        meta["domain"] = domain

                    meta = _sanitize_meta(meta)

                    try:
                        collection.add(documents=[chunk_text], ids=[chunk_id], metadatas=[meta])
                        indexed_chunks += 1
                    except Exception as e:
                        print(f"‚ùå Erreur d'ajout d'une section depuis {file}, section {i} : {e}\n[DEBUG meta]={meta}")

        total_files += 1

    print(f"‚úÖ {indexed_chunks} sections index√©es √† partir de {total_files} fichiers.")

