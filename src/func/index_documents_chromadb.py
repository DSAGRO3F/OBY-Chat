"""
Module d'indexation des documents de sant√© dans une base vectorielle ChromaDB.

Ce module prend en entr√©e des fichiers JSON repr√©sentant soit des documents issus de fichiers DOCX,
soit des pages web structur√©es, puis les segmente et les ins√®re dans une collection ChromaDB.
"""



from uuid import uuid4
from datetime import datetime
import os, json
from chromadb.api import ClientAPI
from chromadb.utils import embedding_functions
from src.utils.chroma_client import get_chroma_client


def _collection_name_for(source_type: str) -> str:
    if source_type not in {"docx", "web"}:
        raise ValueError(f"source_type invalide: {source_type!r} (attendus: 'docx' ou 'web')")
    return "base_docx" if source_type == "docx" else "base_web"

def rebuild_collection_from_disk(
    client: ClientAPI,
    source_type: str,
    source_dir: str,
    drop_collection: bool = False,
) -> None:
    """
    Reconstruit la collection ChromaDB associ√©e √† `source_type` √† partir des fichiers
    pr√©sents dans `source_dir`.
    On veut reconstruire une base propre sans fichiers qui auraient d√ªs √™tre supprim√©s du fait
    de leur suppression sur disque dans des s√©quences pr√©c√©dentes.

    Deux strat√©gies :
      - drop_collection=True  : supprime ENTIEREMENT la collection, puis la recr√©e via index_documents(...)
      - drop_collection=False : nettoie uniquement les documents de ce `source_type` (delete where),
                                puis r√©indexe depuis disque via index_documents(...)

    Cette fonction suppose que `index_documents(source_dir, source_type, client)` :
      - (re)cr√©e la collection si n√©cessaire,
      - upsert les documents avec un metadatas["source_type"] = source_type.
    """

    collection_name = _collection_name_for(source_type)

    if drop_collection:
        try:
            client.delete_collection(name=collection_name)
            print(f"üß® Collection supprim√©e : {collection_name}")
        except Exception as e:
            print(f"‚ÑπÔ∏è Suppression ignor√©e (collection absente ?) : {e}")
        # R√©indexation compl√®te depuis le r√©pertoire
        index_documents(source_dir=source_dir, source_type=source_type, client=client)
        print(f"‚úÖ Collection reconstruite : {collection_name}")
        return

    # Variante ¬´ soft ¬ª : on garde la collection mais on supprime tous les documents du type concern√©
    try:
        embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-large-en-v1.5"
        )
        collection = client.get_or_create_collection(
            name=collection_name, embedding_function=embedding_fn
        )
        try:
            before = collection.count()
        except Exception:
            before = None

        # On supprime tous les documents de ce source_type (m√™me si une seule collection est partag√©e)
        collection.delete(where={"source_type": source_type})
        try:
            after = collection.count()
            print(f"‚ú≥Ô∏è Nettoyage {collection_name} (source_type={source_type}) : {before} ‚Üí {after}")
        except Exception:
            print(f"‚ú≥Ô∏è Nettoyage {collection_name} (source_type={source_type}) effectu√©.")

    except Exception as e:
        # Si le nettoyage cibl√© √©choue (collection corrompue, sch√©ma chang√©, etc.), on supprime la collection.
        print(f"‚ö†Ô∏è √âchec du nettoyage cibl√© ({collection_name}). Fallback drop. D√©tail: {e}")
        try:
            client.delete_collection(name=collection_name)
            print(f"‚ùé Collection supprim√©e : {collection_name}")
        except Exception as e2:
            print(f"‚ùå Impossible de supprimer {collection_name} : {e2}")

    finally:
        # Dans tous les cas, on (re)indexe depuis les fichiers pr√©sents
        index_documents(source_dir=source_dir, source_type=source_type, client=client)
        print(f"‚úÖ Collection r√©index√©e depuis le disque : {collection_name}")



def index_documents(source_dir: str, source_type: str, client: ClientAPI):
    """
    Indexe les documents JSON contenus dans un r√©pertoire dans une collection ChromaDB.

    Chaque document est d√©coup√© en sections (ou chunk unique dans le cas d'un fichier DOCX complet),
    puis ins√©r√© dans une base vectorielle avec ses m√©tadonn√©es.

    Args:
        source_dir (str): Chemin du dossier contenant les fichiers JSON √† indexer.
        source_type (str): Type de document √† indexer, soit 'docx' soit 'web'.
        client (Client): Instance du client ChromaDB utilis√©e pour la persistance des donn√©es.

    Entr√©es :
        - source_dir (str) : Dossier contenant les fichiers JSON.
        - source_type (str) : 'docx' ou 'web' (d√©termine la collection cible).

    Sorties :
        - Indexation des chunks dans une collection nomm√©e selon la source.


    Raises:
        ValueError: Si le type de source est invalide (autre que 'docx' ou 'web').
    """

    print("üü° Lancement fonction -> index_documents()... ")
    if client is None:
        client = get_chroma_client()

    if source_type not in ("docx", "web"):
        raise ValueError("source_type doit √™tre 'docx' ou 'web'.")

    collection_name = "base_docx" if source_type == "docx" else "base_web"

    # üîπ Initialisation collection
    print(f'üü°Initialisation de la collection {collection_name}...')
    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="BAAI/bge-large-en-v1.5"
    )
    collection = client.get_or_create_collection(name=collection_name, embedding_function=embedding_fn)

    print(f"üü°Indexation des documents depuis : {source_dir} (type: {source_type})")

    total_files, indexed_chunks = 0, 0

    for file in os.listdir(source_dir):
        if not file.endswith(".json"):
            continue
        print(f"üü° Traitement du fichier JSON : {file}")

        file_path = os.path.join(source_dir, file)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"‚ùå Erreur de lecture {file}: {e}")
            continue

        fiches = data if isinstance(data, list) else [data]

        for fiche in fiches:
            print(
                f"‚û°Ô∏è Titre fiche : {fiche.get('titre', 'Sans titre')} - Source : {fiche.get('source_url', 'inconnue')}")
            titre = fiche.get("titre", "Titre inconnu")
            type_document = fiche.get("type_document", "document")
            source = fiche.get("source_doc" if source_type == "docx" else "source_url", "Source inconnue")

            if source_type == "docx":
                # ‚úÖ 1 seul chunk : texte complet
                chunk_text = fiche.get("texte_complet", "").strip()
                if not chunk_text:
                    print(f"‚ö†Ô∏è Texte vide pour le fichier {file}, source_type : {source_type}")
                    continue

                chunk_id = f"{file}_{uuid4().hex[:8]}"
                try:
                    collection.add(
                        documents=[chunk_text],
                        ids=[chunk_id],
                        metadatas=[{
                            "titre": titre,
                            "type_document": type_document,
                            "source": source,
                            "section_index": 0,
                            "source_type": source_type,
                            "date_indexation": datetime.now().strftime("%Y-%m-%d"),
                        }]
                    )
                    indexed_chunks += 1
                except Exception as e:
                    print(f"‚ùå Erreur d‚Äôajout depuis {file} : {e}")


            elif source_type == "web":
                sections = fiche.get("sections", [])
                if not sections:
                    print(f"‚ö†Ô∏è Aucune section trouv√©e dans le fichier {file}, source_type : {source_type}")
                    continue

                for i, section in enumerate(sections):
                    chunk_text = section.get("texte", "").strip()
                    if not chunk_text:
                        print(f"‚ö†Ô∏è Chunk vide dans section {i} du fichier {file}")
                        continue

                    chunk_id = f"{file}_{uuid4().hex[:8]}"
                    try:
                        collection.add(
                            documents=[chunk_text],
                            ids=[chunk_id],
                            metadatas=[{
                                "titre": titre,
                                "type_document": type_document,
                                "source": source,
                                "section_index": i,
                                "source_type": source_type,
                                "date_indexation": datetime.now().strftime("%Y-%m-%d"),
                            }]
                        )
                        indexed_chunks += 1
                    except Exception as e:
                        print(f"‚ùå Erreur d'ajout d'une section depuis {file}, section {i} : {e}")


        total_files += 1

    print(f"‚úÖ {indexed_chunks} sections index√©es √† partir de {total_files} fichiers.")

