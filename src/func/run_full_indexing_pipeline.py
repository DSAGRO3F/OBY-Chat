"""
Module `run_full_indexing_pipeline.py` ‚Äì Pipeline principal d‚Äôindexation documentaire pour OBY-IA.

Ce module ex√©cute l‚Äôensemble du processus de pr√©paration de la base documentaire utilis√©e
par les agents RAG de OBY-IA, en assurant une indexation vectorielle actualis√©e dans ChromaDB.

Fonctionnalit√©s couvertes :
1. **D√©tection de modifications** :
   - Identification des fichiers DOCX ou pages web r√©cemment modifi√©s via calcul de hashs.
   - D√©tection des changements dans la d√©finition des sites de confiance (`trusted_sites.py`).

2. **Conversion en JSON structur√©** :
   - Transformation des fichiers DOCX en fichiers JSON exploitables.
   - Scraping et structuration des nouvelles pages web selon les r√®gles d√©finies.

3. **Indexation vectorielle dans ChromaDB** :
   - Indexation incr√©mentale ou compl√®te des donn√©es selon les changements d√©tect√©s.
   - S√©paration des sources DOCX et web (`source_type`).

4. **Journalisation des indexations** :
   - Mise √† jour du fichier de suivi (`indexed_files.json`) pour √©viter les r√©indexations inutiles.

5. **Signalement de disponibilit√©** :
   - √âcriture d‚Äôun fichier `index_ready.flag` permettant aux autres modules de savoir si l‚Äôindex est pr√™t.

Ce pipeline peut √™tre lanc√© :
- automatiquement (via un scheduler ou watchdog),
- ou manuellement (en ex√©cutant ce fichier en tant que script).

Il constitue un composant critique du syst√®me OBY-IA pour garantir la fra√Æcheur et la coh√©rence
des bases documentaires utilis√©es dans les interactions LLM + RAG.
"""


from pathlib import Path
import os, sys

from src.utils.convert_fiches_docx_to_json import convert_and_save_fiches
from config.config import INPUT_DOCX, JSON_HEALTH_DOC_BASE, WEB_SITES_JSON_HEALTH_DOC_BASE
from src.func.indexed_health_related_files import (
    detect_changes_and_get_modified_files,
    update_index_journal,
)
from src.func.scrape_trusted_sites import scrape_all_trusted_sites
from src.func.index_documents_chromadb import index_documents
from src.utils.chroma_client import get_chroma_client
from src.utils.vector_db_utils import mark_index_ready_flag, clear_index_ready_flag
from src.func.index_documents_chromadb import _collection_name_for, rebuild_collection_from_disk


def run_full_indexing_pipeline():
    """
    Ex√©cute le pipeline complet d‚Äôindexation des documents m√©dicaux.

    Ce pipeline effectue les √©tapes suivantes :
    1. D√©tection des fichiers modifi√©s (DOCX, JSON web, fichier des sites de confiance).
    2. Conversion des fichiers DOCX en JSON.
    3. Scraping et structuration des pages web si n√©cessaire.
    4. Indexation vectorielle des fichiers convertis (DOCX et web) dans ChromaDB.
    5. Mise √† jour du journal des fichiers index√©s.

    Ce processus permet d'assurer que la base documentaire est √† jour pour les requ√™tes RAG.
    """

    clear_index_ready_flag()
    print("[DEBUG ‚úÖ] clear_index_ready_flag() appel√©")

    print("üü° Lancement du pipeline d'indexation...")
    print(f"üü° Dossier d'entr√©e DOCX : {INPUT_DOCX}")

    # D√©tection des changements
    changes_dict = detect_changes_and_get_modified_files()
    docx_files_to_index = changes_dict.get("docx_files_to_index", [])
    web_files_to_index = changes_dict.get("web_files_to_index", [])
    trusted_sites_changed = changes_dict.get("trusted_sites_py_changed", False)
    current_docx_hashes = changes_dict.get("current_docx_hashes", {})
    current_web_hashes = changes_dict.get("current_web_hashes", {})
    current_py_hash = changes_dict.get("current_py_hash", None)
    docx_deleted_files = changes_dict.get("docx_deleted_files", [])
    web_deleted_files = changes_dict.get("web_deleted_files", [])


    print(f"üü† DOCX √† indexer : {len(docx_files_to_index)} fichiers")
    print(f"üü† WEB √† indexer : {len(web_files_to_index)} fichiers")
    print(f"üü† Files trusted_sites.py modifi√© ? {trusted_sites_changed}")

    # Si un fichier docx est supprim√©, suppression de son √©quivalent dans json
    docx_json_deleted = False
    for docx_path in docx_deleted_files:
        json_candidate = Path(JSON_HEALTH_DOC_BASE) / (Path(docx_path).stem + ".json")
        try:
            os.remove(json_candidate)
            print(f"‚úÖ JSON d√©riv√© supprim√© (DOCX supprim√©) : {json_candidate}")
            docx_json_deleted = True
        except FileNotFoundError:
            # Si rien fichier d√©j√† absent, on poursuit
            pass
        except Exception as e:
            print(f"‚ùå Impossible de supprimer {json_candidate} : {e}", file=sys.stderr)



    # DOCX : D√©tection de fichiers DOCX + conversion en JSON et sauvegarde
    if not current_docx_hashes:
        print("‚ö†Ô∏è Aucun fichier DOCX d√©tect√© (aucune conversion √† faire).")
    else:
        if docx_files_to_index:
            print("‚úÖ Conversion des fiches DOCX modifi√©es...")
            for docx_file in docx_files_to_index:
                print(f"üü° docx_file: {docx_file}")
                convert_and_save_fiches(str(docx_file), JSON_HEALTH_DOC_BASE)
        else:
            print("üü° Aucun DOCX modifi√© ‚Äî conversion non n√©cessaire.")



    # WEB : D√©tection & scraping si n√©cessaire si pas de json ou modif. liste sites web
    web_content_changed = False  # pour d√©cider de (r√©)indexer ou non
    if not current_web_hashes:
        # Aucun JSON web encore pr√©sent -> scraping initial
        print("‚ö†Ô∏è Aucun fichier JSON web d√©tect√© ‚Äî scraping initial des sources de confiance...")
        scrape_all_trusted_sites()
        web_content_changed = True

    elif trusted_sites_changed:
        # La config des sites a chang√© -> on rescrape
        print("üü° La liste des sites de confiance a chang√© ‚Äî scraping complet...")
        scrape_all_trusted_sites()
        web_content_changed = True

    elif web_files_to_index:
        # Des JSON web ont √©t√© modifi√©s/ajout√©s depuis le dernier journal
        # (ex: scraping pr√©c√©dent, ajout manuel, etc.) -> pas besoin de rescraper,
        print(f"üü° {len(web_files_to_index)} fichier(s) JSON web modifi√©(s) ‚Äî scraping non n√©cessaire.")
        web_content_changed = True

    else:
        print("‚úÖ Aucun changement web ‚Äî ni scraping ni r√©indexation n√©cessaires.")



    # Construction index vectoriel si n√©cessaire
    needs_reindex = (
            bool(docx_files_to_index) or
            bool(web_content_changed) or
            bool(docx_deleted_files) or
            bool(web_deleted_files)
    )

    if needs_reindex:
        print("üü° Construction d'un nouvel index vectoriel...")
        client = get_chroma_client()

        # DOCX
        rebuild_collection_from_disk(
            client=client,
            source_type="docx",
            source_dir=JSON_HEALTH_DOC_BASE,
            drop_collection=False,  # passe √† True si tu veux un drop complet
        )

        # WEB
        rebuild_collection_from_disk(
            client=client,
            source_type="web",
            source_dir=WEB_SITES_JSON_HEALTH_DOC_BASE,
            drop_collection=False,  # idem
        )
    else:
        print("‚úÖ Aucun changement d√©tect√©, indexation non n√©cessaire.")


    # Mise √† jour du journal des fichiers
    post_changes = detect_changes_and_get_modified_files()
    update_index_journal(
        new_docx_hashes=post_changes.get("current_docx_hashes", {}),
        new_web_hashes=post_changes.get("current_web_hashes", {}),
        new_py_hash=post_changes.get("current_py_hash", None),
    )
    print("‚úÖ Journal d'index mis √† jour.")
    print("‚úÖ Pipeline termin√© avec succ√®s !")


    mark_index_ready_flag()
    print("[DEBUG ‚úÖ] mark_index_ready_flag() appel√©")


# Pour ex√©cution directe:
if __name__ == "__main__":
    run_full_indexing_pipeline()
    mark_index_ready_flag()



