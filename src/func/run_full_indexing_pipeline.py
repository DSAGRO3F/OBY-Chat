"""
Module `run_full_indexing_pipeline.py` â€“ Pipeline principal dâ€™indexation documentaire pour OBY-IA.

Ce module exÃ©cute lâ€™ensemble du processus de prÃ©paration de la base documentaire utilisÃ©e
par les agents RAG de OBY-IA, en assurant une indexation vectorielle actualisÃ©e dans ChromaDB.

FonctionnalitÃ©s couvertes :
1. **DÃ©tection de modifications** :
   - Identification des fichiers DOCX ou pages web rÃ©cemment modifiÃ©s via calcul de hashs.
   - DÃ©tection des changements dans la dÃ©finition des sites de confiance (`trusted_sites.py`).

2. **Conversion en JSON structurÃ©** :
   - Transformation des fichiers DOCX en fichiers JSON exploitables.
   - Scraping et structuration des nouvelles pages web selon les rÃ¨gles dÃ©finies.

3. **Indexation vectorielle dans ChromaDB** :
   - Indexation incrÃ©mentale ou complÃ¨te des donnÃ©es selon les changements dÃ©tectÃ©s.
   - SÃ©paration des sources DOCX et web (`source_type`).

4. **Journalisation des indexations** :
   - Mise Ã  jour du fichier de suivi (`indexed_files.json`) pour Ã©viter les rÃ©indexations inutiles.

5. **Signalement de disponibilitÃ©** :
   - Ã‰criture dâ€™un fichier `index_ready.flag` permettant aux autres modules de savoir si lâ€™index est prÃªt.

Ce pipeline peut Ãªtre lancÃ© :
- automatiquement (via un scheduler ou watchdog),
- ou manuellement (en exÃ©cutant ce fichier en tant que script).

Il constitue un composant critique du systÃ¨me OBY-IA pour garantir la fraÃ®cheur et la cohÃ©rence
des bases documentaires utilisÃ©es dans les interactions LLM + RAG.
"""




from src.utils.convert_fiches_docx_to_json import convert_and_save_fiches
from config.config import INPUT_DOCX, JSON_HEALTH_DOC_BASE, WEB_SITES_JSON_HEALTH_DOC_BASE
from pathlib import Path
from src.func.indexed_health_related_files import (
    detect_changes_and_get_modified_files,
    update_index_journal,
)
from src.func.scrape_trusted_sites import scrape_all_trusted_sites
from src.func.index_documents_chromadb import index_documents
from src.utils.chroma_client import get_chroma_client
from src.utils.vector_db_utils import mark_index_ready_flag, clear_index_ready_flag


def run_full_indexing_pipeline():
    """
    ExÃ©cute le pipeline complet dâ€™indexation des documents mÃ©dicaux.

    Ce pipeline effectue les Ã©tapes suivantes :
    1. DÃ©tection des fichiers modifiÃ©s (DOCX, JSON web, fichier des sites de confiance).
    2. Conversion des fichiers DOCX en JSON.
    3. Scraping et structuration des pages web si nÃ©cessaire.
    4. Indexation vectorielle des fichiers convertis (DOCX et web) dans ChromaDB.
    5. Mise Ã  jour du journal des fichiers indexÃ©s.

    Ce processus permet d'assurer que la base documentaire est Ã  jour pour les requÃªtes RAG.
    """

    clear_index_ready_flag()
    print("[DEBUG âœ…] clear_index_ready_flag() appelÃ©")

    print("ğŸŸ¡ Lancement du pipeline d'indexation...")
    print(f"ğŸŸ¡ Dossier d'entrÃ©e DOCX : {INPUT_DOCX}")

    # DÃ©tection des changements
    changes_dict = detect_changes_and_get_modified_files()
    docx_files_to_index = changes_dict.get("docx_files_to_index", [])
    web_files_to_index = changes_dict.get("web_files_to_index", [])
    trusted_sites_changed = changes_dict.get("trusted_sites_py_changed", False)
    current_docx_hashes = changes_dict.get("current_docx_hashes", {})
    current_web_hashes = changes_dict.get("current_web_hashes", {})
    current_py_hash = changes_dict.get("current_py_hash", None)

    print(f"ğŸŸ  DOCX Ã  indexer : {len(docx_files_to_index)} fichiers")
    print(f"ğŸŸ  WEB Ã  indexer : {len(web_files_to_index)} fichiers")
    print(f"ğŸŸ  Files trusted_sites.py modifiÃ© ? {trusted_sites_changed}")



    # DOCX : DÃ©tection de fichiers DOCX + conversion en JSON et sauvegarde
    if not current_docx_hashes:
        print("âš ï¸ Aucun fichier DOCX dÃ©tectÃ© (aucune conversion Ã  faire).")
    else:
        if docx_files_to_index:
            print("âœ… Conversion des fiches DOCX modifiÃ©es...")
            for docx_file in docx_files_to_index:
                print(f"ğŸŸ¡ docx_file: {docx_file}")
                convert_and_save_fiches(str(docx_file), JSON_HEALTH_DOC_BASE)
        else:
            print("ğŸŸ¡ Aucun DOCX modifiÃ© â€” conversion non nÃ©cessaire.")



    # WEB : DÃ©tection & scraping si nÃ©cessaire si pas de json ou modif. liste sites web
    web_content_changed = False  # pour dÃ©cider de (rÃ©)indexer ou non
    if not current_web_hashes:
        # Aucun JSON web encore prÃ©sent -> scraping initial
        print("âš ï¸ Aucun fichier JSON web dÃ©tectÃ© â€” scraping initial des sources de confiance...")
        scrape_all_trusted_sites()
        web_content_changed = True

    elif trusted_sites_changed:
        # La config des sites a changÃ© -> on rescrape
        print("ğŸŸ¡ La liste des sites de confiance a changÃ© â€” scraping complet...")
        scrape_all_trusted_sites()
        web_content_changed = True

    elif web_files_to_index:
        # Des JSON web ont Ã©tÃ© modifiÃ©s/ajoutÃ©s depuis le dernier journal
        # (ex: scraping prÃ©cÃ©dent, ajout manuel, etc.) -> pas besoin de rescraper,
        print(f"ğŸŸ¡ {len(web_files_to_index)} fichier(s) JSON web modifiÃ©(s) â€” scraping non nÃ©cessaire.")
        web_content_changed = True

    else:
        print("âœ… Aucun changement web â€” ni scraping ni rÃ©indexation nÃ©cessaires.")


    # Construction index vectoriel si nÃ©cessaire
    needs_reindex = bool(docx_files_to_index) or bool(web_content_changed)

    if needs_reindex:
        print("ğŸŸ¡ Construction d'un nouvel index vectoriel...")
        client = get_chroma_client()
        index_documents(JSON_HEALTH_DOC_BASE, source_type="docx", client=client)
        index_documents(WEB_SITES_JSON_HEALTH_DOC_BASE, source_type="web", client=client)
    else:
        print("âœ… Aucun changement dÃ©tectÃ©, indexation non nÃ©cessaire.")


    # Mise Ã  jour du journal des fichiers
    post_changes = detect_changes_and_get_modified_files()
    update_index_journal(
        new_docx_hashes=post_changes.get("current_docx_hashes", {}),
        new_web_hashes=post_changes.get("current_web_hashes", {}),
        new_py_hash=post_changes.get("current_py_hash", None),
    )
    print("âœ… Journal d'index mis Ã  jour.")



    # update_index_journal(
    #     new_docx_hashes=current_docx_hashes,
    #     new_web_hashes=current_web_hashes,
    #     new_py_hash=current_py_hash,
    # )
    print("âœ… Pipeline terminÃ© avec succÃ¨s !")


    mark_index_ready_flag()
    print("[DEBUG âœ…] mark_index_ready_flag() appelÃ©")


# Pour exÃ©cution directe:
if __name__ == "__main__":
    run_full_indexing_pipeline()
    mark_index_ready_flag()



