"""
    Module `run_full_indexing_pipeline.py` ‚Äì Pipeline principal d‚Äôindexation documentaire pour OBY-IA.

    Pipeline d'indexation ChromaDB pour OBY-IA.

    Ce module orchestre la maintenance de l‚Äôindex vectoriel √† partir de deux sources :
    1) des fiches au format DOCX (converties en JSON),
    2) des pages web de confiance (scrap√©es en JSON).

    Il a pour objectif d'√™tre appel√© au d√©marrage et √† chaque √©v√©nement Watchdog.

    Fonctionnement, synth√®se :
    - D√©tection des changements via `detect_changes_and_get_modified_files()` :
      ajouts, modifications, suppressions de fichiers DOCX/WEB, changement de
      `trusted_web_sites_list.py`.
    - Nettoyage :
      - suppression des JSON d√©riv√©s de DOCX supprim√©s,
      - purge d√©fensive des JSON web si la configuration des sites change.
    - Production des donn√©es :
      - conversion DOCX ‚Üí JSON si des DOCX ont chang√©,
      - scraping complet/partiel des sites web si n√©cessaire.
    - Reconstruction des index ChromaDB :
      - r√©indexation des collections √† partir des dossiers JSON pr√©sents sur disque.
    - Mise √† jour du journal et pose d‚Äôun ¬´ ready flag ¬ª.

    D√©pendances (import√©es ailleurs dans le projet) :
    - `detect_changes_and_get_modified_files`, `update_index_journal`
    - `convert_and_save_fiches`
    - `scrape_all_trusted_sites`
    - `get_chroma_client`, `index_documents` (ou `rebuild_collection_from_disk`)
    - constantes de chemins : `INPUT_DOCX`, `JSON_HEALTH_DOC_BASE`,
      `WEB_SITES_JSON_HEALTH_DOC_BASE`, `WEB_SITES_MODULE_PATH`, `BASE_DIR`

    Notes :
    - Les purges de r√©pertoires sont pr√©c√©d√©es de v√©rifications de chemin
      (r√©solution absolue, inclusion sous `BASE_DIR`).
    - Les erreurs critiques d‚ÄôE/S sont logu√©es sur STDERR.

"""

from pathlib import Path
import os, sys, shutil

from src.utils.convert_fiches_docx_to_json import convert_and_save_fiches
from config.config import INPUT_DOCX, JSON_HEALTH_DOC_BASE, WEB_SITES_JSON_HEALTH_DOC_BASE, BASE_DIR
from src.func.indexed_health_related_files import (
    detect_changes_and_get_modified_files,
    update_index_journal,
)
from src.func.scrape_trusted_sites import scrape_all_trusted_sites
from src.utils.chroma_client import get_chroma_client
from src.utils.vector_db_utils import mark_index_ready_flag, clear_index_ready_flag
from src.func.index_documents_chromadb import rebuild_collection_from_disk


def run_full_indexing_pipeline():
    """
    Ex√©cute le pipeline complet de supervision et (r√©)indexation.

    Objectifs :
        1. D√©tecte l‚Äô√©tat courant et les diffs (ajouts/modifs/suppressions).
        2. Supprime les JSON orphelins issus de DOCX supprim√©s.
        3. Si la configuration des sites change, purge les JSON web puis lance
           un scraping complet ; sinon, scraping conditionnel si n√©cessaire.
        4. Reconstruit l‚Äôindex ChromaDB √† partir des JSON pr√©sents sur disque
           (DOCX et WEB), si des changements ont √©t√© d√©tect√©s.
        5. Recalcule les hachages et met √† jour le journal d‚Äôindexation.
        6. Pose le ¬´ ready flag ¬ª marquant la fin r√©ussie du processus.

    Notes :
        - √âcrit/√©crase des fichiers JSON (conversion DOCX, scraping web).
        - Purge de dossiers JSON (web) en cas de changement de configuration.
        - (R√©)initialise des collections ChromaDB.
        - Met √† jour le journal d‚Äôindexation et le drapeau ¬´ ready ¬ª.

    Raises:
        RuntimeError: si une incoh√©rence de chemin est d√©tect√©e lors d‚Äôune purge.
        OSError: en cas d‚Äôerreurs E/S non g√©r√©es par les ¬´ ignore_errors ¬ª.
        Exception: toutes exceptions non intercept√©es par les appels sous-jacents.

    Returns:
        None
    """

    clear_index_ready_flag()
    print("[DEBUG ‚úÖ] clear_index_ready_flag() appel√©")

    print("üü° Lancement du pipeline d'indexation...")
    print(f"üü° Dossier d'entr√©e DOCX + nb files: {INPUT_DOCX}, {len(INPUT_DOCX)}")

    # D√©tection des changements
    changes_dict = detect_changes_and_get_modified_files()
    docx_files_to_index = changes_dict.get("docx_files_to_index", [])
    web_files_to_index = changes_dict.get("web_files_to_index", [])
    trusted_sites_changed = changes_dict.get("trusted_sites_py_changed", False)
    current_docx_hashes = changes_dict.get("current_docx_hashes", {})
    current_web_hashes = changes_dict.get("current_web_hashes", {})
    current_py_hash = changes_dict.get("current_py_hash", None)
    docx_deleted_files = changes_dict.get("docx_deleted_files", [])
    web_deleted_files = changes_dict.get("web_deleted_files", [])


    print(f"üü† DOCX √† indexer : {len(docx_files_to_index)} fichiers")
    print(f"üü† WEB √† indexer : {len(web_files_to_index)} fichiers")
    print(f"üü† Files trusted_sites.py modifi√© ? {trusted_sites_changed}")

    # Si un fichier docx est supprim√©, suppression de son √©quivalent dans json
    docx_json_deleted = False
    for docx_path in docx_deleted_files:
        json_candidate = Path(JSON_HEALTH_DOC_BASE) / (Path(docx_path).stem + ".json")
        try:
            os.remove(json_candidate)
            print(f"‚úÖ JSON d√©riv√© supprim√© (DOCX supprim√©) : {json_candidate}")
            docx_json_deleted = True
        except FileNotFoundError:
            # Si rien fichier d√©j√† absent, on poursuit
            pass
        except Exception as e:
            print(f"‚ùå Impossible de supprimer {json_candidate} : {e}", file=sys.stderr)


    # Si un fichier web est supprim√©, suppression de son √©quivalent dans json
    web_json_deleted = False
    for web_json_path in web_deleted_files:  # √©l√©ments de type Path pointant vers WEB_SITES_JSON_HEALTH_DOC_BASE/xxx.json
        try:
            if web_json_path.exists():
                os.remove(web_json_path)
                print(f"‚úÖ JSON web supprim√© (journal signale une suppression) : {web_json_path}")
                web_json_deleted = True
        except Exception as e:
            print(f"‚ùå Impossible de nettoyer {web_json_path} : {e}", file=sys.stderr)


    # DOCX : D√©tection de fichiers DOCX + conversion en JSON et sauvegarde
    if not current_docx_hashes:
        print("‚ö†Ô∏è Aucun fichier DOCX d√©tect√© (aucune conversion √† faire).")
    else:
        if docx_files_to_index:
            print("‚úÖ Conversion des fiches DOCX modifi√©es...")
            for docx_file in docx_files_to_index:
                print(f"üü° docx_file: {docx_file}")
                convert_and_save_fiches(str(docx_file), JSON_HEALTH_DOC_BASE)
        else:
            print("üü° Aucun DOCX modifi√© ‚Äî conversion non n√©cessaire.")



    # WEB : D√©tection & scraping si n√©cessaire si pas de json ou modif. de la liste sites web
    web_content_changed = False  # pour d√©cider de (r√©)indexer ou non
    if not current_web_hashes:
        # Aucun JSON web encore pr√©sent -> scraping initial
        print("‚ö†Ô∏è Aucun fichier JSON web d√©tect√© ‚Äî scraping initial des sources de confiance...")
        scrape_all_trusted_sites()
        web_content_changed = True

    elif trusted_sites_changed:
        # La config des sites a chang√© implique nouveau scrape
        print("üü° La liste des sites de confiance a chang√© ‚Äî scraping complet...")

        # on v√©rifie que path (chemin √† purger) commence par le chemin absolu de BASE_DIR.
        # => On ne purge que si la cible est bien dans le r√©pertoire racine attendu.
        # Si pas le cas ‚Üí ‚ÄúChemin inattendu, purge annul√©e‚Äù.



        base = Path(BASE_DIR).resolve()
        path = WEB_SITES_JSON_HEALTH_DOC_BASE  # str ou Path vers le dossier √† purger
        target = Path(path).resolve()

        # V√©rifie que target == base ou est un sous-dossier de base
        if not (target == base or base in target.parents):
            raise RuntimeError(f"Chemin inattendu (hors {base}) : purge annul√©e ‚Üí {target}")


        shutil.rmtree(WEB_SITES_JSON_HEALTH_DOC_BASE, ignore_errors=True)
        os.makedirs(WEB_SITES_JSON_HEALTH_DOC_BASE, exist_ok=True)
        scrape_all_trusted_sites()
        web_content_changed = True

    elif web_files_to_index:
        # Des JSON web ont √©t√© modifi√©s/ajout√©s depuis le dernier journal
        # (ex: scraping pr√©c√©dent, ajout manuel, etc.) -> pas besoin nouveau scrape
        print(f"üü° {len(web_files_to_index)} fichier(s) JSON web modifi√©(s) ‚Äî scraping non n√©cessaire.")
        web_content_changed = True

    else:
        print("‚úÖ Aucun changement web ‚Äî ni scraping ni r√©indexation n√©cessaires.")


    # Construction index vectoriel si n√©cessaire

    needs_reindex = (
            bool(docx_files_to_index)
            or bool(web_content_changed)
            or bool(docx_deleted_files)
            or bool(web_deleted_files)
            or bool(docx_json_deleted)
            or bool(web_json_deleted)
    )


    if needs_reindex:
        print("üü° Reconstruction de l'index vectoriel...")
        client = get_chroma_client()
        rebuild_collection_from_disk(client, "docx", JSON_HEALTH_DOC_BASE)
        rebuild_collection_from_disk(client, "web", WEB_SITES_JSON_HEALTH_DOC_BASE)
    else:
        print("‚úÖ Aucun changement d√©tect√©, indexation non n√©cessaire.")


    # Mise √† jour du journal des fichiers
    post_changes = detect_changes_and_get_modified_files()
    update_index_journal(
        new_docx_hashes=post_changes.get("current_docx_hashes", {}),
        new_web_hashes=post_changes.get("current_web_hashes", {}),
        new_py_hash=post_changes.get("current_py_hash", None),
    )
    print("‚úÖ Journal d'index mis √† jour.")
    print("‚úÖ Pipeline termin√© avec succ√®s !")


    mark_index_ready_flag()
    print("[DEBUG ‚úÖ] mark_index_ready_flag() appel√©")


# Pour ex√©cution directe:
if __name__ == "__main__":
    run_full_indexing_pipeline()
    mark_index_ready_flag()



