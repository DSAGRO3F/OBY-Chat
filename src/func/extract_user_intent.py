"""
Module extract_user_intent

Ce module permet de d√©tecter l‚Äôintention principale d‚Äôun utilisateur √† partir de sa requ√™te textuelle.
La d√©tection repose d‚Äôabord sur des correspondances par mots-cl√©s, puis bascule sur un mod√®le de langage
(LLM) si aucune correspondance directe n‚Äôest trouv√©e.

Modifications apport√©es :
- Ajout d‚Äôune hi√©rarchie de priorit√© dans la d√©tection par mots-cl√©s pour r√©soudre les ambigu√Øt√©s.
- Ajout de docstrings conformes √† la PEP 257.
- Refactorisation avec √©tapes explicites et commentaires clairs.
"""

from typing import Dict, Literal
from langchain_core.language_models import BaseLanguageModel
from src.llm_user_session.model import llm_model


IntentType = Literal["generate_ppa", "get_constants", "generate_recommendations", "unknown"]

# üîë Mots-cl√©s associ√©s √† chaque intention
intent_keywords = {
    "generate_ppa": [
        "plan d'accompagnement", "ppa", "g√©n√©rer un ppa", "objectifs et actions", "proposer un plan",
        "√©laborer un ppa", "plan personnalis√©", "pr√©parer le plan", "faire un plan pour le patient",
        "cr√©er un plan", "r√©diger le plan", "plan √† mettre en place", "plan √† faire",
        "pr√©vois un accompagnement", "plan du patient", "mettre en place le suivi",
        "organiser le plan de soins", "d√©finir un plan", "construire le plan"
    ],

    "get_constants": [
        "constantes", "tension", "poids", "temp√©rature", "fr√©quence cardiaque", "graphique",
        "affiche les constantes", "√©volution", "valeurs biologiques", "donn√©es de sant√©",
        "historique m√©dical", "voir les courbes", "montre moi les constantes", "valeurs mesur√©es",
        "graphique du patient", "√©tat de sant√©", "r√©sum√© m√©dical", "suivi des constantes",
        "affiche les mesures", "statistiques de sant√©", "courbe de poids", "tendance m√©dicale"
    ],

    "generate_recommendations": [
        "conduite √† tenir", "prise en charge", "recommandations", "quoi faire", "traitement",
        "agir", "soins √† apporter", "pr√©vention", "risque", "urgence", "aide pour un cas",
        "prise de d√©cision", "avis m√©dical", "diagnostic", "que faire", "quelle d√©marche suivre",
        "comment r√©agir", "que dois-je faire", "orientation m√©dicale", "d√©cision m√©dicale",
        "protocole √† suivre", "r√©ponse adapt√©e", "conseils de soins", "suggestion d‚Äôaction",
        "mesures √† prendre", "plan d‚Äôaction √† suivre"
    ]
}

# ‚öñÔ∏è Priorit√© des intentions (plus haut = plus prioritaire)
intent_priority = {
    "get_constants": 1,
    "generate_ppa": 2,
    "generate_recommendations": 3
}


# Fonction de normalisation du texte
import unicodedata
import re

def normalize_text(text: str) -> str:
    """
    Nettoie et standardise une cha√Æne de texte en fran√ßais pour faciliter la d√©tection d‚Äôintention.

    √âtapes :
    - Passage en minuscules
    - Remplacement des apostrophes typographiques
    - Suppression des accents
    - Suppression des caract√®res non alphanum√©riques utiles
    - Nettoyage des espaces multiples

    Args:
        text (str): Texte √† normaliser

    Returns:
        str: Texte nettoy√© et standardis√©
    """
    if not text:
        return ""

    # Minuscule
    text = text.lower()

    # Uniformisation des apostrophes et caract√®res typographiques
    text = text.replace("‚Äô", "'").replace("‚Äò", "'").replace("`", "'")

    # Suppression des accents (ex : √© ‚Üí e)
    text = ''.join(
        c for c in unicodedata.normalize('NFD', text)
        if unicodedata.category(c) != 'Mn'
    )

    # Suppression des caract√®res sp√©ciaux inutiles sauf apostrophes et chiffres
    text = re.sub(r"[^a-z0-9' ]", " ", text)

    # Remplacement des espaces multiples par un seul
    text = re.sub(r"\s+", " ", text)

    # Trim final
    return text.strip()



def detect_user_intent(user_input: str) -> Dict[str, str]:
    """
    D√©tecte l‚Äôintention de l‚Äôutilisateur √† partir de sa requ√™te textuelle.

    La fonction effectue :
    1. Une d√©tection par mots-cl√©s avec hi√©rarchie de priorit√© si plusieurs intentions sont d√©tect√©es.
    2. Un fallback par mod√®le de langage (LLM) si aucune correspondance n‚Äôest trouv√©e.

    Args:
        user_input (str): Requ√™te utilisateur.

    Returns:
        dict: Dictionnaire contenant une seule cl√© "intent" avec l‚Äôune des valeurs suivantes :
              "generate_ppa", "get_constants", "generate_recommendations", ou "unknown".
    """
    if not user_input:
        return {"intent": "unknown"}

    user_input_norm = normalize_text(user_input)

    # üîç √âtape 1 : Recherche des intentions par mots-cl√©s
    matched_intents = []
    for intent, keywords in intent_keywords.items():
        for keyword in keywords:
            if keyword in user_input_norm:
                matched_intents.append(intent)
                break  # Un seul mot-cl√© suffit √† activer une intention

    # ‚öñÔ∏è √âtape 2 : S√©lection de l‚Äôintention dominante par priorit√©
    if matched_intents:
        selected_intent = max(matched_intents, key=lambda i: intent_priority[i])
        print(f"üîç Intention d√©tect√©e par mot-cl√© : {selected_intent}")
        return {"intent": selected_intent}

    # üß† √âtape 3 : Fallback ‚Äì classification via LLM
    print("üß† Aucune correspondance simple trouv√©e, appel au LLM pour classification...")
    response = llm_intent_classification(user_input)

    # ‚úÖ V√©rification du r√©sultat du LLM
    if response in intent_priority:
        return {"intent": response}

    print("‚ö†Ô∏è Intention non reconnue apr√®s passage au LLM.")
    return {"intent": "unknown"}



def llm_intent_classification(user_input: str, llm: BaseLanguageModel = None) -> IntentType:
    """
    Utilise un mod√®le de langage (LLM) pour inf√©rer l‚Äôintention utilisateur si aucun mot-cl√© ne correspond.

    Args:
        user_input (str): Requ√™te de l‚Äôutilisateur.
        llm (BaseLanguageModel, optional): Mod√®le LLM. Si None, le mod√®le par d√©faut est utilis√©.

    Returns:
        IntentType: Intention d√©tect√©e ("generate_ppa", "get_constants", "generate_recommendations", ou "unknown").
    """
    if llm is None:
        llm = llm_model

    # üîß Prompt syst√®me renforc√© pour guider le mod√®le
    system_prompt = """
Tu es un assistant expert en classification d‚Äôintentions pour une application appel√©e OBY-IA.

L‚Äôutilisateur peut √©crire en langage naturel comme s‚Äôil s‚Äôadressait √† ChatGPT. Ta t√¢che est de lire sa requ√™te et de d√©terminer l‚Äôintention principale √† d√©clencher.

Voici les intentions disponibles :

- generate_ppa : si l‚Äôutilisateur veut g√©n√©rer un Plan Personnalis√© d‚ÄôAccompagnement (PPA) √† partir d‚Äôun document patient.
- get_constants : si l‚Äôutilisateur veut afficher les constantes m√©dicales (ex : tension, temp√©rature, poids, fr√©quence cardiaque, graphiques‚Ä¶).
- generate_recommendations : si l‚Äôutilisateur veut obtenir des recommandations m√©dicales, une conduite √† tenir, un traitement, ou savoir comment agir.
- unknown : si la requ√™te est trop floue ou ne correspond √† aucun de ces cas.

Tu dois r√©pondre **uniquement** avec le mot-cl√© correspondant, sans ponctuation, sans phrase, sans commentaire.

Exemples valides : generate_ppa / get_constants / generate_recommendations / unknown

Phrase utilisateur :
{user_input}

Intention :
""".strip()

    prompt = system_prompt.format(user_input=user_input.strip())

    try:
        response = llm.invoke(prompt).content.strip().lower()
        print(f"ü§ñ Intention d√©tect√©e via LLM : {response}")
        return response if response in intent_priority else "unknown"
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur LLM dans la d√©tection d‚Äôintention : {e}")
        return "unknown"








# ==================================================
# Code insuffisant pour d√©tection intention correcte
# ==================================================
# def llm_intent_classification(user_input: str, llm: BaseLanguageModel = None) -> IntentType:
#     """
#     Utilise un mod√®le de langage (LLM) pour inf√©rer l‚Äôintention utilisateur si aucun mot-cl√© ne correspond.
#
#     Args:
#         user_input (str): Texte de l‚Äôutilisateur.
#         llm (BaseLanguageModel, optional): Mod√®le √† utiliser. Si None, un mod√®le local est charg√©.
#
#     Returns:
#         IntentType: Intention d√©tect√©e, ou "unknown".
#     """
#     if llm is None:
#         llm = llm_model
#
#     # üßæ Prompt syst√®me pour le classificateur d‚Äôintentions
#     system_prompt = """
# Tu es un classificateur d'intentions. Ton travail est de lire une phrase d'utilisateur et de d√©terminer l‚Äôintention parmi les choix suivants :
# - generate_ppa
# - get_constants
# - generate_recommendations
#
# Si aucune intention ne correspond, retourne "unknown".
#
# R√©ponds uniquement avec le mot-cl√© de l‚Äôintention.
# """
#
#     prompt = f"{system_prompt}\n\nPhrase utilisateur : {user_input}\n\nIntention :"
#
#     try:
#         response = llm.invoke(prompt).content.strip().lower()
#         print(f"ü§ñ Intention d√©tect√©e via LLM : {response}")
#         return response if response in intent_priority else "unknown"
#     except Exception as e:
#         print(f"‚ö†Ô∏è Erreur LLM dans la d√©tection d‚Äôintention : {e}")
#         return "unknown"
#





# ==================================================
# Code insuffisant pour d√©tection intention correcte
# ==================================================
# IntentType = Literal["generate_ppa", "get_constants", "generate_recommendations", "unknown"]
#
# # ‚úÖ Dictionnaire de mots-cl√©s associ√©s √† chaque intention
# intent_keywords = {
#     "generate_ppa": [
#         "plan d'accompagnement", "ppa", "g√©n√©rer un ppa", "objectifs et actions", "proposer un plan",
#         "√©laborer un ppa", "plan personnalis√©"
#     ],
#     "get_constants": [
#         "constantes", "tension", "poids", "temp√©rature", "fr√©quence cardiaque", "graphique",
#         "affiche les constantes", "√©volution", "valeurs biologiques"
#     ],
#     "generate_recommendations": [
#         "conduite √† tenir", "prise en charge", "recommandations", "quoi faire", "traitement",
#         "agir", "soins √† apporter", "pr√©vention", "risque", "urgence", "aide pour un cas",
#         "prise de d√©cision", "avis m√©dical", "diagnostic"
#     ],
# }
#
#
# def detect_user_intent(user_input: str) -> Dict[str, str]:
#     """
#     D√©tecte l‚Äôintention de l‚Äôutilisateur √† partir de sa requ√™te.
#
#     Effectue d'abord une recherche par mots-cl√©s, puis utilise un LLM si aucune correspondance n‚Äôest trouv√©e.
#
#     Args:
#         user_input (str): Requ√™te de l‚Äôutilisateur sous forme de texte.
#
#     Returns:
#         dict: Dictionnaire contenant une cl√© "intent" avec la valeur correspondante.
#               Exemple : {"intent": "get_constants"}
#     """
#     if not user_input:
#         return {"intent": "unknown"}
#
#     user_input_lower = user_input.lower()
#
#     # üîç √âtape 1 : Matching par mots-cl√©s simples
#     for intent, keywords in intent_keywords.items():
#         for keyword in keywords:
#             if keyword in user_input_lower:
#                 print(f"üîç Intention d√©tect√©e par mot-cl√© : {intent}")
#                 print(f'‚úîÔ∏ètype for intent output: {type(intent)}')
#                 return {"intent": intent}
#
#     # üîç √âtape 2 : Fallback ‚Äì classification LLM
#     print("üß† Aucune correspondance simple trouv√©e, appel au LLM pour classification...")
#     response = llm_intent_classification(user_input)
#
#     if response is None:
#         print("‚ö†Ô∏è Le LLM n‚Äôa retourn√© aucune intention. Valeur None intercept√©e.")
#         return {"intent": "unknown"}
#
#     if response in ["generate_ppa", "get_constants", "generate_recommendations"]:
#         return {"intent": response}
#
#     print(f'type for intent output: {type(response)}')
#
#     return {"intent": "unknown"}
#
#
# def llm_intent_classification(user_input: str, llm: BaseLanguageModel = None) -> IntentType:
#     """
#     Utilise un LLM pour inf√©rer l'intention quand aucun mot-cl√© ne correspond.
#
#     Args:
#         user_input (str): Message de l'utilisateur.
#         llm (BaseLanguageModel, optional): Mod√®le LangChain. Par d√©faut = mod√®le local.
#
#     Returns:
#         IntentType: Intention inf√©r√©e.
#     """
#     if llm is None:
#         llm = llm_model()
#
#     system_prompt = """
# Tu es un classificateur d'intentions. Ton travail est de lire une phrase d'utilisateur et de d√©terminer l‚Äôintention parmi les choix suivants :
# - generate_ppa
# - get_constants
# - generate_recommendations
#
# Si aucune intention ne correspond, retourne "unknown".
#
# R√©ponds uniquement avec le mot-cl√© de l‚Äôintention.
# """
#     prompt = f"{system_prompt}\n\nPhrase utilisateur : {user_input}\n\nIntention :"
#
#     try:
#         response = llm.invoke(prompt).content.strip().lower()
#         print(f"ü§ñ Intention d√©tect√©e via LLM : {response}")
#         if response in ["generate_ppa", "get_constants", "generate_recommendations"]:
#             return response  # type: ignore
#     except Exception as e:
#         print(f"‚ö†Ô∏è Erreur LLM dans la d√©tection d'intention : {e}")
#
#     return "unknown"
