"""
Module de gÃ©nÃ©ration de PPA (Plan PersonnalisÃ© dâ€™Accompagnement) Ã  partir dâ€™un document POA.

Ce module extrait le nom du patient depuis la requÃªte utilisateur, charge et nettoie le document POA
correspondant, anonymise les donnÃ©es, formate le contenu pour le modÃ¨le LLM, puis gÃ©nÃ¨re un PPA structurÃ©.
"""





from src.func.poa_loader import load_patient_file
from src.func.poa_cleaning import clean_patient_document
from src.func.detect_poa_file_path import extract_relevant_info
from src.llm_user_session.model import llm_model
from src.func.extract_patient_name import extract_patient_name_llm
from src.func.anonymizer import anonymize_fields
from src.func.anonymizer import deanonymize_fields
from src.utils.convert_json_text import convert_json_to_text
from src.func.llm_prompts import llm_prompt_template_medical_plan, medical_prompt_template, medical_response_from_llm

import tiktoken
import json



def process_ppa_request(user_input, system_prompt):
    """
    Traite une requÃªte utilisateur pour gÃ©nÃ©rer un Plan PersonnalisÃ© dâ€™Accompagnement (PPA) Ã  partir dâ€™un fichier POA.

    Ã‰tapes :
    - Extraction du nom du patient depuis la requÃªte.
    - Chargement et nettoyage du fichier POA associÃ©.
    - Anonymisation des champs sensibles.
    - Conversion du document nettoyÃ© en texte pour le LLM.
    - Construction du prompt et gÃ©nÃ©ration de la rÃ©ponse par le modÃ¨le.
    - DÃ©sanonymisation du rÃ©sultat final.

    Args:
        user_input (str): La requÃªte formulÃ©e par l'utilisateur.
        system_prompt (str): Le prompt systÃ¨me servant de base au modÃ¨le.

    Returns:
        tuple:
            - str: La rÃ©ponse du modÃ¨le dÃ©sanonymisÃ©e, formatÃ©e comme un PPA.
            - dict: Le dictionnaire de mapping dâ€™anonymisation utilisÃ©.
    """

    # 1. Extraire le nom du patient
    patient_name = extract_patient_name_llm(user_input)
    if not patient_name:
        return "âŒ Impossible de dÃ©terminer le nom du patient Ã  partir de la requÃªte."

    print(f"ğŸŸ¢ Recherche du fichier avec patient_name = '{patient_name}'")

    # 2. RÃ©cupÃ©rer le chemin du fichier patient
    print(f'patient_name: {patient_name}')
    patient_file_path = extract_relevant_info(patient_name)
    print(f'patient_file_path: {patient_file_path}')
    if not patient_file_path:
        return f"âŒ Fichier du patient {patient_name} introuvable."

    # 3. Charger le fichier patient
    try:
        raw_document = load_patient_file(patient_file_path)
        if not raw_document:
            print('No raw_document available')
    except FileNotFoundError:
        return f"âŒ Impossible de charger le fichier du patient {patient_name}."
    print("âœ… Chargement du document terminÃ©.")
    print("ğŸ” AperÃ§u du JSON brut :", json.dumps(raw_document, indent=2, ensure_ascii=False)[:1000])

    # 4. Nettoyer le contenu du fichier (POA)
    if not raw_document:
        print('No raw_document available')
    cleaned_document = clean_patient_document(raw_document)
    print("âœ… Document nettoyÃ©.")
    # print("ğŸ” AperÃ§u du document nettoyÃ© :", json.dumps(cleaned_document, indent=2, ensure_ascii=False)[:1000])
    print("ğŸ” AperÃ§u du document nettoyÃ© :", json.dumps(cleaned_document, indent=2, ensure_ascii=False)[:])

    # 4. Bis Anonymisation du POA + conversion dictionnaire en texte
    anonymized_text, dict_mapping = anonymize_fields(cleaned_document)
    print("âœ… Anonymisation effectuÃ©e.")
    # print("ğŸ” Texte anonymisÃ© :", json.dumps(anonymized_text, indent=2, ensure_ascii=False)[:1000])
    print("ğŸ” Texte anonymisÃ© :", json.dumps(anonymized_text, indent=2, ensure_ascii=False)[:])

    print("ğŸ“Œ Exemple de mapping :", list(dict_mapping.items())[:10])

    anonymized_text = convert_json_to_text(anonymized_text)
    print("âœ… Conversion JSON â†’ texte rÃ©ussie.")
    print("ğŸ” Prompt envoyÃ© au modÃ¨le :", anonymized_text)

    #5. encapsule le prompt de base pour ce type d'analyse mÃ©dicale.
    user_prompt_template = llm_prompt_template_medical_plan()

    #6. construire dynamiquement d'autres prompts.
    prompt_template = medical_prompt_template(system_prompt, user_prompt_template)

    print("ğŸŸ¢ Envoi de la requÃªte au LLM...")
    # 7. centralise l'appel au modÃ¨le avec un StrOutputParser
    response = medical_response_from_llm(prompt_template, user_input, anonymized_text)



    # ============================

    # NOMBRE TOKENS ENVOYES AU LLM
    # 1. Formatage du prompt final avec les variables
    final_prompt = prompt_template.format(
        user_input=user_input,
        poa_content=anonymized_text
    )

    # 2. Encodage avec tiktoken (compatible avec GPT, Mistralâ€¦ selon ton modÃ¨le)
    enc = tiktoken.get_encoding("cl100k_base")  # ou adapte selon ton tokenizer
    num_tokens = len(enc.encode(final_prompt))

    print("ğŸ§  Nombre rÃ©el de tokens envoyÃ©s au LLM :", num_tokens)

    # ============================


    # response = llm_model.invoke(final_prompt)
    print(f"ğŸŸ¢ RÃ©ponse brute du modÃ¨le : {response}")
    deanonymized_response, reverse_mapping = deanonymize_fields(response, dict_mapping)

    # 9. Extraction propre du contenu
    return deanonymized_response.strip(), dict_mapping




