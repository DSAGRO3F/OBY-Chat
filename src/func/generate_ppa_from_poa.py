"""
Module de g√©n√©ration de PPA (Plan Personnalis√© d‚ÄôAccompagnement) √† partir d‚Äôun document POA.

Ce module extrait le nom du patient depuis la requ√™te utilisateur, charge et nettoie le document POA
correspondant, anonymise les donn√©es, formate le contenu pour le mod√®le LLM, puis g√©n√®re un PPA structur√©.
"""





from src.func.poa_loader import load_patient_file
from src.func.poa_cleaning import clean_patient_document
from src.func.detect_poa_file_path import extract_relevant_info
from src.llm_user_session.model import llm_model
from src.func.extract_patient_name import extract_patient_name_llm
from src.func.anonymizer import anonymize_patient_document
from src.func.anonymizer import deanonymize_fields
from src.utils.convert_json_text import convert_json_to_text
from src.func.llm_prompts import llm_prompt_template_medical_plan, medical_prompt_template, medical_response_from_llm
from src.func.free_text_name_anonymizer import anonymize_name_mentions_in_free_text

import tiktoken
import json



def process_ppa_request(user_input, system_prompt):
    """
    Traite une requ√™te utilisateur pour g√©n√©rer un Plan Personnalis√© d‚ÄôAccompagnement (PPA) √† partir d‚Äôun fichier POA.

    √âtapes :
    - Extraction du nom du patient depuis la requ√™te.
    - Chargement et nettoyage du fichier POA associ√©.
    - Anonymisation des champs sensibles.
    - Conversion du document nettoy√© en texte pour le LLM.
    - Construction du prompt et g√©n√©ration de la r√©ponse par le mod√®le.
    - D√©sanonymisation du r√©sultat final.

    Args:
        user_input (str): La requ√™te formul√©e par l'utilisateur.
        system_prompt (str): Le prompt syst√®me servant de base au mod√®le.

    Returns:
        tuple:
            - str: La r√©ponse du mod√®le d√©sanonymis√©e, format√©e comme un PPA.
            - dict: Le dictionnaire de mapping d‚Äôanonymisation utilis√©.
    """

    # 1. Extraire le nom du patient
    patient_name = extract_patient_name_llm(user_input)
    if not patient_name:
        return "‚ùå Impossible de d√©terminer le nom du patient √† partir de la requ√™te."

    print(f"üü¢ Recherche du fichier avec patient_name = '{patient_name}'")

    # 2. R√©cup√©rer le chemin du fichier patient
    print(f'patient_name: {patient_name}')
    patient_file_path = extract_relevant_info(patient_name)
    print(f'patient_file_path: {patient_file_path}')
    if not patient_file_path:
        return f"‚ùå Fichier du patient {patient_name} introuvable."

    # 3. Charger le fichier patient
    try:
        raw_document = load_patient_file(patient_file_path)
        if not raw_document:
            print('No raw_document available')
    except FileNotFoundError:
        return f"‚ùå Impossible de charger le fichier du patient {patient_name}."
    print("‚úÖ Chargement du document termin√©.")
    # print("üîç Aper√ßu du JSON brut :", json.dumps(raw_document, indent=2, ensure_ascii=False)[:1000])

    # 4. Nettoyer le contenu du fichier (POA)
    if not raw_document:
        print('No raw_document available')
    cleaned_document = clean_patient_document(raw_document)
    print("‚úÖ Document nettoy√©.")
    print("üüß Aper√ßu du document nettoy√©-avant anony. :", json.dumps(cleaned_document, indent=2, ensure_ascii=False)[0:1500])

    # --- test
    pp = cleaned_document.get("usager", {}) \
        .get("Informations d'√©tat civil", {}) \
        .get("personnePhysique", {})
    print("[AFTER CLEAN] situationFamiliale:",
          "PRESENT" if "situationFamiliale" in pp else "ABSENTE",
          "->", pp.get("situationFamiliale", "<NO KEY>"))

    print("üüß Aper√ßu (global) :", json.dumps(cleaned_document, indent=2, ensure_ascii=False)[0:3000])
    # ---

    # print("üüß Aper√ßu du document nettoy√© :", json.dumps(cleaned_document, indent=2, ensure_ascii=False)[:])

    # 4. Bis Anonymisation du POA + conversion dictionnaire en texte
    anonymized_doc, dict_mapping = anonymize_patient_document(cleaned_document, debug=False)
    anonymized_doc, dict_mapping = anonymize_name_mentions_in_free_text(anonymized_doc, dict_mapping, debug=False)

    print("‚úÖ Anonymisation effectu√©e.")
    print("üüß Texte anonymis√© :", json.dumps(anonymized_doc, indent=2, ensure_ascii=False)[0:1500])
    # print("üüß Texte anonymis√© :", json.dumps(anonymized_text, indent=2, ensure_ascii=False)[:])

    print("üüß Exemple de mapping :", list(dict_mapping.items())[:15])

    # print(f"üüß Apr√®s anonymisation -> {anonymized_doc}")

    anonymized_text = convert_json_to_text(anonymized_doc)
    print("‚úÖ Conversion JSON ‚Üí texte r√©ussie.")
    # print("üîç Prompt envoy√© au mod√®le :", anonymized_text)

    #5. encapsule le prompt de base pour ce type d'analyse m√©dicale.
    user_prompt_template = llm_prompt_template_medical_plan()

    #6. construire dynamiquement d'autres prompts.
    prompt_template = medical_prompt_template(system_prompt, user_prompt_template)

    print("üü¢ Envoi de la requ√™te au LLM...")
    # 7. centralise l'appel au mod√®le avec un StrOutputParser
    response = medical_response_from_llm(prompt_template, user_input, anonymized_text)



    # ============================

    # NOMBRE TOKENS ENVOYES AU LLM
    # 1. Formatage du prompt final avec les variables
    final_prompt = prompt_template.format(
        user_input=user_input,
        poa_content=anonymized_text
    )

    # 2. Encodage avec tiktoken (compatible avec GPT, Mistral‚Ä¶ selon ton mod√®le)
    enc = tiktoken.get_encoding("cl100k_base")  # ou adapte selon ton tokenizer
    num_tokens = len(enc.encode(final_prompt))

    print(" üü¢Nombre r√©el de tokens envoy√©s au LLM :", num_tokens)

    # ============================


    # response = llm_model.invoke(final_prompt)
    print(f"üü¢ R√©ponse brute du mod√®le avt. d√©-anonym. : {response}")
    deanonymized_response, reverse_mapping = deanonymize_fields(response, dict_mapping, debug=True)



    # ============================

    # NOMBRE TOKENS RECUS DU LLM
    # Comptage des tokens dans la r√©ponse du LLM
    num_output_tokens = len(enc.encode(response))
    print(" üü¢Nombre r√©el de tokens re√ßus du LLM :", num_output_tokens)

    # ============================


    # 9. Extraction propre du contenu
    return deanonymized_response.strip(), dict_mapping




